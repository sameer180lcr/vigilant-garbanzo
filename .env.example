# Muse Executive Deployment Configuration
# -------------------------------------

# The public URL of your Ollama API (or OpenAI compatible API)
# For local development, leave this as http://localhost:11434
# For production (Render/Vercel), point this to your remote inference server.
VITE_OLLAMA_API=http://localhost:11434

# Note: Ensure your remote API has CORS enabled for your deployment domain.
